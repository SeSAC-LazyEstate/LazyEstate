{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "704dac88-8eb6-4a69-a1c7-e7dc9d4f2a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Model RMSE with Important Features: 31212.121312050167\n",
      "Best Model R-squared (R2) with Important Features: 0.9938363734932162\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def calculate_rmse(targets, predictions):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "\n",
    "def calculate_R2_score(y_test, y_pred):\n",
    "    from sklearn.metrics import r2_score\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# 데이터 불러오기\n",
    "input_data = pd.read_csv('2023_smartFarm_AI_hackathon_dataset.csv')\n",
    "\n",
    "# 농가구역 One-hot encoding\n",
    "input_data = pd.get_dummies(input_data, columns=['frmDist'], drop_first=False)\n",
    "\n",
    "# na값 drop\n",
    "input_data = input_data.dropna()\n",
    "\n",
    "# 특성과 타겟 변수 설정\n",
    "X = input_data.drop(columns=['outtrn_cumsum', 'HeatingEnergyUsage_cumsum'])\n",
    "y = input_data[['outtrn_cumsum', 'HeatingEnergyUsage_cumsum']]\n",
    "\n",
    "# 트레이닝, 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Extra Trees Regressor 모델\n",
    "model = ExtraTreesRegressor(n_estimators=50, random_state=42)  # n_estimators 값을 줄임\n",
    "\n",
    "# 하이퍼파라미터 그리드 설정 (RandomizedSearchCV 사용)\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150],  # 더 작은 값을 시도\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV 객체 생성\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, n_jobs=-1, verbose=1, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "# RandomizedSearchCV 수행\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 최적의 모델 선택\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# 중요한 특성만 선택 (임계값 조정 필요)\n",
    "threshold = 0.01  # 중요도의 임계값\n",
    "important_features = X_train.columns[best_model.feature_importances_ > threshold]\n",
    "X_train_important = X_train_scaled[:, best_model.feature_importances_ > threshold]\n",
    "X_test_important = X_test_scaled[:, best_model.feature_importances_ > threshold]\n",
    "\n",
    "# 모델 재훈련\n",
    "best_model.fit(X_train_important, y_train)\n",
    "\n",
    "# 테스트 데이터로 예측\n",
    "y_pred = best_model.predict(X_test_important)\n",
    "\n",
    "# 평가\n",
    "rmse = calculate_rmse(y_test, y_pred)\n",
    "r2score = calculate_R2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model RMSE with Important Features:\", rmse)\n",
    "print(\"Best Model R-squared (R2) with Important Features:\", r2score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "120d4094-d047-49eb-a501-042ebaa13dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.1.0.tar.gz (1.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/sopung/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from lightgbm) (1.11.2)\n",
      "Requirement already satisfied: numpy in /Users/sopung/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from lightgbm) (1.24.3)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for lightgbm \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[44 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2023-09-16 23:36:42,924 - scikit_build_core - INFO - CMake version: 3.27.4\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.5.0\u001b[0m using \u001b[94mCMake 3.27.4\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2023-09-16 23:36:42,926 - scikit_build_core - INFO - Build directory: /private/var/folders/jm/t447cbrj0rv_y73nfrlz2_fr0000gn/T/tmpeor09vht/build\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2023-09-16 23:36:43,175 - scikit_build_core - INFO - Ninja version: 1.11.1\n",
      "  \u001b[31m   \u001b[0m loading initial cache file /var/folders/jm/t447cbrj0rv_y73nfrlz2_fr0000gn/T/tmpeor09vht/build/CMakeInit.txt\n",
      "  \u001b[31m   \u001b[0m \u001b[0mCMake Deprecation Warning at CMakeLists.txt:35 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   Compatibility with CMake < 3.5 will be removed from a future version of\n",
      "  \u001b[31m   \u001b[0m   CMake.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  \u001b[31m   \u001b[0m   CMake that the project does not need compatibility with older versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m -- The C compiler identification is AppleClang 14.0.0.14000029\n",
      "  \u001b[31m   \u001b[0m -- The CXX compiler identification is AppleClang 14.0.0.14000029\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND)\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_C: -Xpreprocessor -fopenmp -I/opt/homebrew/opt/libomp/include\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_CXX: -Xpreprocessor -fopenmp -I/opt/homebrew/opt/libomp/include\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP: TRUE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_PREFETCH\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_PREFETCH - Failed\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_MALLOC\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_MALLOC - Success\n",
      "  \u001b[31m   \u001b[0m -- Using _mm_malloc\n",
      "  \u001b[31m   \u001b[0m -- Configuring done (0.7s)\n",
      "  \u001b[31m   \u001b[0m -- Generating done (0.0s)\n",
      "  \u001b[31m   \u001b[0m -- Build files have been written to: /var/folders/jm/t447cbrj0rv_y73nfrlz2_fr0000gn/T/tmpeor09vht/build\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mBuilding project with \u001b[94mNinja\u001b[0m...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m ninja: error: '/opt/homebrew/opt/libomp/lib/libomp.dylib', needed by '/private/var/folders/jm/t447cbrj0rv_y73nfrlz2_fr0000gn/T/pip-install-7ufol717/lightgbm_ecd8ddeb14b14d27b47e2e9fa305fed3/lib_lightgbm.so', missing and no known rule to make it\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[91m\u001b[1m*** CMake build failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for lightgbm\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build lightgbm\n",
      "\u001b[31mERROR: Could not build wheels for lightgbm, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/sopung/.pyenv/versions/3.9.13/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12d8b4c-8f05-4791-8791-32959d62c46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1697/1697 [==============================] - 1s 584us/step - loss: 101563392000.0000 - val_loss: 65744478208.0000\n",
      "Epoch 2/50\n",
      "1697/1697 [==============================] - 1s 534us/step - loss: 59316101120.0000 - val_loss: 62018842624.0000\n",
      "Epoch 3/50\n",
      "1697/1697 [==============================] - 1s 533us/step - loss: 56592441344.0000 - val_loss: 60276023296.0000\n",
      "Epoch 4/50\n",
      "1697/1697 [==============================] - 1s 535us/step - loss: 55121195008.0000 - val_loss: 58832437248.0000\n",
      "Epoch 5/50\n",
      "1697/1697 [==============================] - 1s 551us/step - loss: 53414572032.0000 - val_loss: 57480728576.0000\n",
      "Epoch 6/50\n",
      "1697/1697 [==============================] - 1s 535us/step - loss: 52607406080.0000 - val_loss: 56543473664.0000\n",
      "Epoch 7/50\n",
      "1697/1697 [==============================] - 1s 551us/step - loss: 51172548608.0000 - val_loss: 55305388032.0000\n",
      "Epoch 8/50\n",
      "1697/1697 [==============================] - 1s 537us/step - loss: 50369798144.0000 - val_loss: 54461313024.0000\n",
      "Epoch 9/50\n",
      "1697/1697 [==============================] - 1s 528us/step - loss: 49666486272.0000 - val_loss: 53813063680.0000\n",
      "Epoch 10/50\n",
      "1697/1697 [==============================] - 1s 535us/step - loss: 49091801088.0000 - val_loss: 53472825344.0000\n",
      "Epoch 11/50\n",
      "1697/1697 [==============================] - 1s 535us/step - loss: 48084721664.0000 - val_loss: 52820434944.0000\n",
      "Epoch 12/50\n",
      "1697/1697 [==============================] - 1s 537us/step - loss: 48195305472.0000 - val_loss: 52629192704.0000\n",
      "Epoch 13/50\n",
      "1697/1697 [==============================] - 1s 532us/step - loss: 47909777408.0000 - val_loss: 52046065664.0000\n",
      "Epoch 14/50\n",
      "1697/1697 [==============================] - 1s 553us/step - loss: 47364710400.0000 - val_loss: 51888214016.0000\n",
      "Epoch 15/50\n",
      "1697/1697 [==============================] - 1s 525us/step - loss: 46908674048.0000 - val_loss: 51497463808.0000\n",
      "Epoch 16/50\n",
      "1697/1697 [==============================] - 1s 527us/step - loss: 47012274176.0000 - val_loss: 51335028736.0000\n",
      "Epoch 17/50\n",
      "1697/1697 [==============================] - 1s 530us/step - loss: 46505431040.0000 - val_loss: 51109101568.0000\n",
      "Epoch 18/50\n",
      "1697/1697 [==============================] - 1s 531us/step - loss: 46540013568.0000 - val_loss: 50943975424.0000\n",
      "Epoch 19/50\n",
      "1697/1697 [==============================] - 1s 529us/step - loss: 46432169984.0000 - val_loss: 50725666816.0000\n",
      "Epoch 20/50\n",
      "1697/1697 [==============================] - 1s 529us/step - loss: 46273454080.0000 - val_loss: 50607243264.0000\n",
      "Epoch 21/50\n",
      "1697/1697 [==============================] - 1s 529us/step - loss: 46161174528.0000 - val_loss: 50503331840.0000\n",
      "Epoch 22/50\n",
      "1697/1697 [==============================] - 1s 528us/step - loss: 45993742336.0000 - val_loss: 50362388480.0000\n",
      "Epoch 23/50\n",
      "1697/1697 [==============================] - 1s 532us/step - loss: 45491154944.0000 - val_loss: 50200293376.0000\n",
      "Epoch 24/50\n",
      "1697/1697 [==============================] - 1s 527us/step - loss: 45816958976.0000 - val_loss: 50067841024.0000\n",
      "Epoch 25/50\n",
      "1697/1697 [==============================] - 1s 529us/step - loss: 45680558080.0000 - val_loss: 50059776000.0000\n",
      "Epoch 26/50\n",
      "1697/1697 [==============================] - 1s 533us/step - loss: 45765349376.0000 - val_loss: 49892106240.0000\n",
      "Epoch 27/50\n",
      "1697/1697 [==============================] - 1s 528us/step - loss: 45192310784.0000 - val_loss: 49774440448.0000\n",
      "Epoch 28/50\n",
      "1697/1697 [==============================] - 1s 527us/step - loss: 45470654464.0000 - val_loss: 49716031488.0000\n",
      "Epoch 29/50\n",
      "1697/1697 [==============================] - 1s 527us/step - loss: 45437034496.0000 - val_loss: 49641381888.0000\n",
      "Epoch 30/50\n",
      "1697/1697 [==============================] - 1s 520us/step - loss: 45467680768.0000 - val_loss: 49838743552.0000\n",
      "Epoch 31/50\n",
      "1697/1697 [==============================] - 1s 526us/step - loss: 45458243584.0000 - val_loss: 49525592064.0000\n",
      "Epoch 32/50\n",
      "1697/1697 [==============================] - 1s 532us/step - loss: 45102669824.0000 - val_loss: 49409052672.0000\n",
      "Epoch 33/50\n",
      "1697/1697 [==============================] - 1s 525us/step - loss: 45025226752.0000 - val_loss: 49325613056.0000\n",
      "Epoch 34/50\n",
      "1697/1697 [==============================] - 1s 542us/step - loss: 44933468160.0000 - val_loss: 49235304448.0000\n",
      "Epoch 35/50\n",
      "1697/1697 [==============================] - 1s 539us/step - loss: 44747890688.0000 - val_loss: 49337208832.0000\n",
      "Epoch 36/50\n",
      "1697/1697 [==============================] - 1s 530us/step - loss: 44976349184.0000 - val_loss: 49140068352.0000\n",
      "Epoch 37/50\n",
      "1697/1697 [==============================] - 1s 525us/step - loss: 45223219200.0000 - val_loss: 49166340096.0000\n",
      "Epoch 38/50\n",
      "1697/1697 [==============================] - 1s 530us/step - loss: 44966514688.0000 - val_loss: 49010810880.0000\n",
      "Epoch 39/50\n",
      "1697/1697 [==============================] - 1s 532us/step - loss: 44502847488.0000 - val_loss: 48925224960.0000\n",
      "Epoch 40/50\n",
      "1697/1697 [==============================] - 1s 537us/step - loss: 44761333760.0000 - val_loss: 48839102464.0000\n",
      "Epoch 41/50\n",
      "1697/1697 [==============================] - 1s 528us/step - loss: 44500185088.0000 - val_loss: 48834510848.0000\n",
      "Epoch 42/50\n",
      "1697/1697 [==============================] - 1s 538us/step - loss: 44561006592.0000 - val_loss: 48737284096.0000\n",
      "Epoch 43/50\n",
      "1697/1697 [==============================] - 1s 535us/step - loss: 44648513536.0000 - val_loss: 48609464320.0000\n",
      "Epoch 44/50\n",
      "1697/1697 [==============================] - 1s 538us/step - loss: 44164943872.0000 - val_loss: 48673382400.0000\n",
      "Epoch 45/50\n",
      "1697/1697 [==============================] - 1s 533us/step - loss: 44486696960.0000 - val_loss: 48504528896.0000\n",
      "Epoch 46/50\n",
      "1697/1697 [==============================] - 1s 541us/step - loss: 44648337408.0000 - val_loss: 48448761856.0000\n",
      "Epoch 47/50\n",
      "1697/1697 [==============================] - 1s 533us/step - loss: 44299988992.0000 - val_loss: 48427315200.0000\n",
      "Epoch 48/50\n",
      "1697/1697 [==============================] - 1s 543us/step - loss: 44318728192.0000 - val_loss: 48306987008.0000\n",
      "Epoch 49/50\n",
      "1697/1697 [==============================] - 1s 547us/step - loss: 44176318464.0000 - val_loss: 48255913984.0000\n",
      "Epoch 50/50\n",
      "1697/1697 [==============================] - 1s 540us/step - loss: 43806834688.0000 - val_loss: 48284835840.0000\n",
      "531/531 [==============================] - 0s 278us/step\n",
      "Deep Learning Model RMSE: 214690.96209225975\n",
      "Deep Learning Model R-squared (R2): 0.5101531948552973\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def calculate_rmse(targets, predictions):\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "\n",
    "def calculate_R2_score(y_test, y_pred):\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# 데이터 불러오기\n",
    "input_data = pd.read_csv('2023_smartFarm_AI_hackathon_dataset.csv')\n",
    "\n",
    "# 농가구역 One-hot encoding\n",
    "input_data = pd.get_dummies(input_data, columns=['frmDist'], drop_first=False)\n",
    "\n",
    "# na값 drop\n",
    "input_data = input_data.dropna()\n",
    "\n",
    "# 특성과 타겟 변수 설정\n",
    "X = input_data.drop(columns=['outtrn_cumsum', 'HeatingEnergyUsage_cumsum'])\n",
    "y = input_data[['outtrn_cumsum', 'HeatingEnergyUsage_cumsum']]\n",
    "\n",
    "# 트레이닝, 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 딥러닝 회귀 모델 정의\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    keras.layers.Dropout(0.2),  # 드롭아웃 추가 (20%의 뉴런을 비활성화)\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),  # 드롭아웃 추가 (20%의 뉴런을 비활성화)\n",
    "    keras.layers.Dense(2)  # 출력 뉴런 수는 예측할 타겟 변수 수와 동일해야 합니다.\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "rmse = calculate_rmse(y_test, y_pred)\n",
    "r2score = calculate_R2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Deep Learning Model RMSE:\", rmse)\n",
    "print(\"Deep Learning Model R-squared (R2):\", r2score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae656963-0871-41d3-9e09-90eb33b2917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 15081.02447539747\n",
      "R2_score: 0.9990819579665828\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# RMSE 계산 함수 정의\n",
    "def calculate_rmse(targets, predictions):\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "\n",
    "# R2 스코어 계산 함수 정의\n",
    "def calculate_R2_score(y_test, y_pred):\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# 데이터 불러오기\n",
    "input_data = pd.read_csv('2023_smartFarm_AI_hackathon_dataset.csv')\n",
    "\n",
    "# 데이터 전처리\n",
    "# 'frmDist' 열을 기준으로 데이터 그룹화 및 평균값 계산하여 'group_df'에 저장\n",
    "group_df = input_data.groupby('frmDist').mean()\n",
    "\n",
    "# 'frmDist' 열을 기준으로 데이터 다시 그룹화하고, 'outtrn_cumsum' 열에서 최댓값 계산하여 'group_outtrn_cumsum_df'에 저장\n",
    "group_outtrn_cumsum_df = input_data.groupby('frmDist').max()[['outtrn_cumsum']]\n",
    "\n",
    "# 'group_df'에서 'outtrn_cumsum'과 'date' 열 제거\n",
    "group_df.drop(['outtrn_cumsum', 'date'], axis=1, inplace=True)\n",
    "\n",
    "# 'group_df'와 'group_outtrn_cumsum_df'를 합쳐 'merge_df' 데이터프레임 생성\n",
    "merge_df = pd.concat([group_df, group_outtrn_cumsum_df], axis=1)\n",
    "\n",
    "# 'merge_df'에서 입력 피처(독립 변수)와 타겟 변수(종속 변수) 분리\n",
    "train_data = merge_df.drop(['outtrn_cumsum', 'HeatingEnergyUsage_cumsum'], axis=1)\n",
    "target_data = merge_df.loc[:,['outtrn_cumsum', 'HeatingEnergyUsage_cumsum']]\n",
    "\n",
    "# 데이터 분할: 훈련 데이터와 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# 데이터 표준화\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "\n",
    "train_scaled = ss.transform(X_train)\n",
    "test_scaled = ss.transform(X_test)\n",
    "\n",
    "# ExtraTreesRegressor 모델 설정 및 하이퍼파라미터 조정\n",
    "et = ExtraTreesRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=10,  # 최대 트리 깊이를 제한\n",
    "    min_samples_split=2,  # 노드를 분할하기 위한 최소 샘플 수\n",
    "    min_samples_leaf=1,  # 리프 노드에 필요한 최소 샘플 수\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "et.fit(train_scaled, y_train)\n",
    "\n",
    "# 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
    "y_pred = et.predict(test_scaled)\n",
    "\n",
    "# RMSE 및 R2 스코어 계산\n",
    "rmse = calculate_rmse(y_test, y_pred) \n",
    "r2score = calculate_R2_score(y_test, y_pred)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2_score:\", r2score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87f6e5b-c48e-4407-b5db-5d6254bc3f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 30195.54486401781\n",
      "R2_score: 0.9643788725840623\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calculate_rmse(targets, predictions):\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "\n",
    "def calculate_R2_score(y_test, y_pred):\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "input_data = pd.read_csv('2023_smartFarm_AI_hackathon_dataset.csv')\n",
    "\n",
    "# 값이 0인 행을 제거하는 대신, 필요한 피처만 추출할 수 있습니다.\n",
    "data_df = input_data[input_data['HeatingEnergyUsage_cumsum'] > 0]\n",
    "\n",
    "group_df = data_df.groupby('frmDist').mean()\n",
    "group_outtrn_cumsum_df = data_df.groupby('frmDist').max()[['outtrn_cumsum']]\n",
    "group_df.drop(['outtrn_cumsum', 'date'], axis=1, inplace=True)\n",
    "merge_df = pd.concat([group_df, group_outtrn_cumsum_df], axis=1)\n",
    "\n",
    "train_data = merge_df.drop(['outtrn_cumsum', 'HeatingEnergyUsage_cumsum'], axis=1)\n",
    "target_data = merge_df.loc[:,['outtrn_cumsum', 'HeatingEnergyUsage_cumsum']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "\n",
    "train_scaled = ss.transform(X_train)\n",
    "test_scaled = ss.transform(X_test)\n",
    "\n",
    "# RandomForestRegressor 모델 설정\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(train_scaled, y_train)\n",
    "\n",
    "y_pred = rf.predict(test_scaled)\n",
    "\n",
    "rmse = calculate_rmse(y_test, y_pred)\n",
    "r2score = calculate_R2_score(y_test, y_pred)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2_score:\", r2score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9c3334-1515-4d34-a15c-67c8d7e69bf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab7eb559-e744-44ff-b015-2281d4b9d310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.1.0.tar.gz (1.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/sopung/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/sopung/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from lightgbm) (1.11.2)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for lightgbm \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[44 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2023-09-17 12:38:49,311 - scikit_build_core - INFO - CMake version: 3.27.4\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.5.0\u001b[0m using \u001b[94mCMake 3.27.4\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2023-09-17 12:38:49,313 - scikit_build_core - INFO - Build directory: /private/var/folders/jm/t447cbrj0rv_y73nfrlz2_fr0000gn/T/tmp648bswb8/build\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2023-09-17 12:38:49,794 - scikit_build_core - INFO - Ninja version: 1.11.1\n",
      "  \u001b[31m   \u001b[0m loading initial cache file /var/folders/jm/t447cbrj0rv_y73nfrlz2_fr0000gn/T/tmp648bswb8/build/CMakeInit.txt\n",
      "  \u001b[31m   \u001b[0m \u001b[0mCMake Deprecation Warning at CMakeLists.txt:35 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   Compatibility with CMake < 3.5 will be removed from a future version of\n",
      "  \u001b[31m   \u001b[0m   CMake.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  \u001b[31m   \u001b[0m   CMake that the project does not need compatibility with older versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m -- The C compiler identification is AppleClang 14.0.0.14000029\n",
      "  \u001b[31m   \u001b[0m -- The CXX compiler identification is AppleClang 14.0.0.14000029\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND)\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_C: -Xpreprocessor -fopenmp -I/opt/homebrew/opt/libomp/include\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_CXX: -Xpreprocessor -fopenmp -I/opt/homebrew/opt/libomp/include\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP: TRUE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_PREFETCH\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_PREFETCH - Failed\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_MALLOC\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_MALLOC - Success\n",
      "  \u001b[31m   \u001b[0m -- Using _mm_malloc\n",
      "  \u001b[31m   \u001b[0m -- Configuring done (0.8s)\n",
      "  \u001b[31m   \u001b[0m -- Generating done (0.0s)\n",
      "  \u001b[31m   \u001b[0m -- Build files have been written to: /var/folders/jm/t447cbrj0rv_y73nfrlz2_fr0000gn/T/tmp648bswb8/build\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mBuilding project with \u001b[94mNinja\u001b[0m...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m ninja: error: '/opt/homebrew/opt/libomp/lib/libomp.dylib', needed by '/private/var/folders/jm/t447cbrj0rv_y73nfrlz2_fr0000gn/T/pip-install-_etoju01/lightgbm_3ebde9746e054a5fae2954816a6b3dfd/lib_lightgbm.so', missing and no known rule to make it\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[91m\u001b[1m*** CMake build failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for lightgbm\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build lightgbm\n",
      "\u001b[31mERROR: Could not build wheels for lightgbm, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/sopung/.pyenv/versions/3.9.13/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
